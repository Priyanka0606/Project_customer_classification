{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "364d42a1",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9b4f5b",
   "metadata": {},
   "source": [
    "Build a Machine Learning Model that classifies customers into high revenue and low revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe788c60",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44894cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle ## helps storing data in pickle files\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af27c2fc",
   "metadata": {},
   "source": [
    "## Setting Display options "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf03e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec3a146",
   "metadata": {},
   "source": [
    "## Warning Suppression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62898e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed972b8",
   "metadata": {},
   "source": [
    "## Import Data for ML Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01117d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "data = pd.read_csv('/Users/priyankac/Downloads/Projects/Existing_Base.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f4c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few records\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3776d1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of rows and columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d4ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check information about the data\n",
    "data.info()\n",
    "\n",
    "## Insight##\n",
    "# Out of 32 columns, we have around 18 columns of numeric type, 14 columns of string type\n",
    "# There doesn't seem to have missing values in the columns, however we would investigate this once again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c33032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Visualizing numeric columns ######\n",
    "\n",
    "numeric_cols = data.select_dtypes(include = np.number) ### selects numeric columns\n",
    "\n",
    "column_names = list(numeric_cols.columns)\n",
    "\n",
    "col_index = 0\n",
    "\n",
    "plot_rows = 6\n",
    "plot_cols = 3\n",
    "\n",
    "fig, ax = plt.subplots(nrows = plot_rows,ncols=plot_cols,figsize = (20,20))\n",
    "\n",
    "for row_count in range(plot_rows):\n",
    "    for col_count in range(plot_cols):\n",
    "        ax[row_count][col_count].scatter(y = numeric_cols[column_names[col_index]],x=numeric_cols.index)\n",
    "        ax[row_count][col_count].set_ylabel(column_names[col_index])\n",
    "        col_index = col_index + 1\n",
    "        \n",
    "###### Insights #####\n",
    "#####################\n",
    "\n",
    "## Ref Number column just contains the index so can be removed.\n",
    "## year_last_moved column seem to have many values close to 0 or 0, these seem to be missing values, investigate them.\n",
    "## Average_credit_card_transaction,Balance Transfer seems to be highly right skewed, check if the skewness could be reduced.\n",
    "## A single customer with very high value of balance transfer/life insurance etc stands out, this row could be considered outlier and be removed,\n",
    "## Capping could be done to limit the impact of outliers.\n",
    "## Scaling could be done since the scale is different for most of the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5ec448",
   "metadata": {},
   "source": [
    "## Drop ID type of feature('REF_NO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921581b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['REF_NO'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73801381",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe71f9f",
   "metadata": {},
   "source": [
    "## Label the Target Feature to 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552428cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the Target Feature\n",
    "data['Revenue Grid'].value_counts()\n",
    "\n",
    "# There is class imbalance in my dataset(9069 customers are low revenue as compared to 1086 high revenue customers)\n",
    "# The distribution is between 80%-20% and 95%-5% -  area of slight concern..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3466d0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'] = np.where(data['Revenue Grid'] == 2, 0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf5c6ed",
   "metadata": {},
   "source": [
    "## Drop the 'Revenue Grid' Feature to retain 'Target' Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Revenue Grid'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3730cd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ab6934",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae203f1e",
   "metadata": {},
   "source": [
    "## Defining the Target and Independent Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df46325",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data[['target']]\n",
    "X = data.drop(['target'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4367fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fd2de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba3a65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a13d714",
   "metadata": {},
   "source": [
    "## Split the features into Numerical and Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2f7f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = X.select_dtypes(include = 'number')\n",
    "char = X.select_dtypes(include = 'object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf8bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e98fc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "char.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f72d7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13144550",
   "metadata": {},
   "outputs": [],
   "source": [
    "char.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c585cb9",
   "metadata": {},
   "source": [
    "## Check Distribution of Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa67ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['Average Credit Card Transaction'],hist = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1e22b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['Balance Transfer'],hist = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdbcaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['Term Deposit'],hist = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8e3a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['Life Insurance'],hist = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2966f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['Medical Insurance'],hist = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5971cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['Average A/C Balance'],hist = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc408bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['Personal Loan'],hist = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956f1445",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['Investment in Mutual Fund'],hist = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b2168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['Investment Tax Saving Bond'],hist = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9576404",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['Home Loan'],hist = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ad795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['Online Purchase Amount'],hist = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bef0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['Portfolio Balance'],hist = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f31fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['Investment in Commudity'],hist = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec36fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['Investment in Equity'],hist = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada7d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['Investment in Derivative'],hist = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ba911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the numeric columns show skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c480e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of unique feature levels in numeric columns\n",
    "def unique_levels(x):\n",
    "    x = x.value_counts().count()\n",
    "    return(x)\n",
    "\n",
    "df_value_counts = pd.DataFrame(num.apply(lambda x : unique_levels(x)))\n",
    "df_value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6623854",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_value_counts.columns = ['feature_levels']\n",
    "df_value_counts\n",
    "\n",
    "# it is observed that faeture level in each feature is more that 25, hence no action to be taken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ddd8d4",
   "metadata": {},
   "source": [
    "## Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b6be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num.describe(percentiles = [0.01,0.05,0.10,0.25,0.50,0.75,0.85,0.9,0.99])\n",
    "\n",
    "# There seems to be huge difference between the 99th percentile and maximum value in most of the features\n",
    "# showing presence of outliers, Capping could be done to limit the impact of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59cd7c4",
   "metadata": {},
   "source": [
    "## Flooring and Capping of Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e509821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_cap(x):\n",
    "    x = x.clip(lower = x.quantile(0.01))\n",
    "    x = x.clip(upper = x.quantile(0.99))\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f224cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = num.apply(lambda x : outlier_cap(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b35b18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num.describe(percentiles = [0.01,0.05,0.10,0.25,0.50,0.75,0.85,0.9,0.99])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf02a0",
   "metadata": {},
   "source": [
    "## Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1b52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f81b293",
   "metadata": {},
   "outputs": [],
   "source": [
    "char.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df9f5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is no missing values in this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a298c310",
   "metadata": {},
   "source": [
    "## Feature Selection - Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e64d7a2",
   "metadata": {},
   "source": [
    "### Part 1 : Remove Features with 0 Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ade410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "varselector = VarianceThreshold(threshold = 0)\n",
    "varselector.fit_transform(num)\n",
    "\n",
    "# Get columns to keep and create new dataframe with those only\n",
    "cols = varselector.get_support(indices = True)\n",
    "num_1 = num.iloc[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea7238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_1.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9267a412",
   "metadata": {},
   "source": [
    "### Part 2 : Bi Variate Analysis(KBinsDiscretizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9dbd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "discrete = KBinsDiscretizer(n_bins = 10, encode = 'ordinal', strategy = 'quantile')\n",
    "num_binned = pd.DataFrame(discrete.fit_transform(num_1), index = num_1.index, \n",
    "                          columns = num_1.columns).add_suffix('_Rank')\n",
    "num_binned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363acf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the features show a slope at all\n",
    "# If they do, then do you see some deciles below the population average and some higher than the population average?\n",
    "# If that is the case then the slope will be strong\n",
    "\n",
    "# Conclusion: A strong slope is indicator of the features' ability to discriminate the event from non event\n",
    "#             making it a good predictor\n",
    "\n",
    "X_bin_combined = pd.concat([Y, num_binned], axis = 1, join = 'inner')\n",
    "\n",
    "from numpy import mean\n",
    "for cols in (num_binned.columns):\n",
    "    plt.figure()\n",
    "    sns.lineplot(x = cols, y = X_bin_combined['target'].mean(), data = X_bin_combined, color = 'red')\n",
    "    sns.barplot(x = cols, y = 'target', data = X_bin_combined, estimator = mean)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19b3736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the above using scatterplot and lineplot\n",
    "for cols in (num_binned.columns):\n",
    "    plt.figure()\n",
    "    sns.scatterplot(x = cols, y = X_bin_combined['target'].mean(), data = X_bin_combined, color = 'red')\n",
    "    sns.lineplot(x = cols, y = 'target', data = X_bin_combined, estimator = mean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11fd403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the feature year_last_moved\n",
    "\n",
    "num = num.drop(['year_last_moved'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bce45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f37829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the features from the num will get selected due to good discrimination power by all of them\n",
    "select_features_df_num = num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce5b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_features_df_num.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95a82bc",
   "metadata": {},
   "source": [
    "## Feature Selection - Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308e310b",
   "metadata": {},
   "source": [
    "### Part 1 : Bi Variate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496cb47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_char_merged = pd.concat([Y, char], axis = 1, join = 'inner')\n",
    "\n",
    "from numpy import mean\n",
    "for col in (char.columns):\n",
    "    plt.figure()\n",
    "    sns.lineplot(x = col, y = X_char_merged['target'].mean(), data = X_char_merged, color = 'red')\n",
    "    sns.barplot(x = col, y = 'target', data = X_char_merged, estimator = mean)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b944d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the features that do no have any slope visible\n",
    "char = char.drop(['TVarea', 'post_code', 'post_area','region'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7bb72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "char.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6678e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy features with n-1 levels\n",
    "X_char_dum = pd.get_dummies(char, drop_first = True)\n",
    "X_char_dum.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3669d29a",
   "metadata": {},
   "source": [
    "### Part 2 : Select KBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310852ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "selector = SelectKBest(chi2, k = 52)\n",
    "selector.fit_transform(X_char_dum, Y)\n",
    "\n",
    "# Get the columns to keep and create new dataframe with those only\n",
    "cols = selector.get_support(indices = True)\n",
    "select_features_df_char = X_char_dum.iloc[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d69c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_features_df_char.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b100423",
   "metadata": {},
   "source": [
    "## Creating the Master Feature Set for Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679226eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = pd.concat([select_features_df_char, select_features_df_num], axis = 1, join = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4021a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b343c4cb",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bab69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, Y, test_size = 0.2, random_state =20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e3b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of Training data : ',X_train.shape)\n",
    "print('Shape pf Testing data : ',X_test.shape)\n",
    "print('Revenue Rate in Training data : ',y_train.mean())\n",
    "print('Revenue Rate in Testing data : ',y_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bcc164",
   "metadata": {},
   "source": [
    "## Model Building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e6fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with fitting the logistic regression model, this would serve as a benchmark model \n",
    "# since Logistic model doesn't have alot of parameters we won't create a validation set for hyperparameter tuning\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR_model = LogisticRegression(class_weight = 'balanced',max_iter=200,random_state=20)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e295117",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52aea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "LR_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7c2016",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = LR_model.predict_proba(X_all)[:, 1]\n",
    "data['pred_prob_logreg'] = pd.DataFrame(y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31bcf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d81cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_train = LR_model.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfbdbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(prediction_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d442719a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(LR_model.predict(X_train)) ### applies a threshold of 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the roc curve for the model fit \n",
    "\n",
    "from sklearn.metrics import roc_auc_score, plot_roc_curve, confusion_matrix,f1_score ## model evaluation metrics\n",
    "plot_roc_curve(LR_model,X =X_train, y= y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301872fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting confusion matrix, F-score on the train data \n",
    "\n",
    "print(confusion_matrix(y_true = y_train, y_pred = LR_model.predict(X_train))) ### confusion matrix for pred on train set\n",
    "\n",
    "print ('The F1-SCORE on the train set prediction ',f1_score(y_true=y_train,y_pred = LR_model.predict(X_train),sample_weight = y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274537d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets evaluate the performace on the test set \n",
    "\n",
    "\n",
    "plot_roc_curve(LR_model,X =X_test, y= y_test)\n",
    "\n",
    "predicted_val = LR_model.predict(X_test)\n",
    "\n",
    "print (confusion_matrix(y_true = y_test, y_pred = predicted_val))\n",
    "\n",
    "print ('The F1-SCORE on the test set prediction ',f1_score(y_true=y_test,y_pred = predicted_val,sample_weight =y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0374fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Decision Tree Model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtree = DecisionTreeClassifier(criterion = 'gini', random_state =20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604c036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GridSearchCV to find the best parameters\n",
    "\n",
    "np.random.seed(44)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_dist = {'max_depth': [6,7,8,9], 'min_samples_split': [50, 100, 150, 200,250]}\n",
    "tree_grid = GridSearchCV(dtree, cv = 10, param_grid = param_dist, n_jobs = -1)\n",
    "tree_grid.fit(X_train, y_train)\n",
    "print('Best parameters using grid search: \\n', tree_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cacbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(criterion = 'gini', random_state =20, max_depth = 6, min_samples_split = 50)\n",
    "dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43797cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating on the train and the test set \n",
    "\n",
    "predicted_train = dtree.predict(X_train)\n",
    "\n",
    "plot_roc_curve(dtree,X =X_train, y= y_train)\n",
    "\n",
    "\n",
    "print(confusion_matrix(y_true = y_train, y_pred = predicted_train))\n",
    "\n",
    "print ('The F1-SCORE on the train set prediction ',f1_score(y_true=y_train,y_pred = predicted_train,sample_weight = y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df1bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(dtree,X =X_test, y= y_test)\n",
    "\n",
    "predicted_test = dtree.predict(X_test)\n",
    "\n",
    "print (confusion_matrix(y_true = y_test, y_pred = predicted_test))\n",
    "\n",
    "print ('The F1-SCORE on the test set prediction ',f1_score(y_true=y_test,y_pred = predicted_test,sample_weight = y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9834543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the feature importances \n",
    "\n",
    "importances = dtree.feature_importances_\n",
    "\n",
    "columns = list(X_train.columns)\n",
    "\n",
    "importances_dict = {columns[i]: importances[i] for i in range(len(columns))}\n",
    "\n",
    "importances_dict = dict(sorted(importances_dict.items(), key=lambda importances_dict: importances_dict[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd77ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting feature importances\n",
    "import pandas as pd\n",
    "feature_importances = pd.DataFrame(dtree.feature_importances_,\n",
    "                                  index = X_train.columns,\n",
    "                                  columns = ['importance']).sort_values('importance', ascending = False)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb4f63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "plot_rows =1\n",
    "plot_cols = 1\n",
    "\n",
    "fig, ax1 = plt.subplots(nrows = plot_rows,ncols=plot_cols,figsize = (20,20))\n",
    "\n",
    "plot_tree(dtree,ax=ax1,fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43455bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(criterion = 'gini', random_state = 20, max_depth = 6, min_samples_split = 50)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e52eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating on the train and the test set \n",
    "\n",
    "predicted_train = rf.predict(X_train)\n",
    "\n",
    "plot_roc_curve(rf,X =X_train, y= y_train)\n",
    "\n",
    "print ('The score for the DT model ', roc_auc_score(y_train,predicted_train))\n",
    "\n",
    "print(confusion_matrix(y_true = y_train, y_pred = predicted_train))\n",
    "\n",
    "print ('The F1-SCORE on the train set prediction ',f1_score(y_true=y_train,y_pred = predicted_train,sample_weight = y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2964e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(rf,X =X_test, y= y_test)\n",
    "\n",
    "predicted_test = rf.predict(X_test)\n",
    "\n",
    "print (confusion_matrix(y_true = y_test, y_pred = predicted_test))\n",
    "\n",
    "print ('The F1-SCORE on the test set prediction ',f1_score(y_true=y_test,y_pred = predicted_test,sample_weight = y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecd87c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting feature importances\n",
    "import pandas as pd\n",
    "feature_importances = pd.DataFrame(rf.feature_importances_,\n",
    "                                  index = X_train.columns,\n",
    "                                  columns = ['importance']).sort_values('importance', ascending = False)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b765e7e",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0edf4d",
   "metadata": {},
   "source": [
    "Logistic Regression Model seemed to give the best f1 score on both train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e608410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db82c71d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b677d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d29d5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a4e87c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23352c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004187e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
